# The LlamaCloud API key.
# LLAMA_CLOUD_API_KEY=

# The provider for the AI models to use.
MODEL_PROVIDER=openai

# The name of LLM model to use.
MODEL=gpt-4o-mini

# Name of the embedding model to use.
EMBEDDING_MODEL=text-embedding-3-small

# Dimension of the embedding model to use.
EMBEDDING_DIM=1536

# The OpenAI API key to use.
# OPENAI_API_KEY=

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# FILESERVER_URL_PREFIX is the URL prefix of the server storing the images generated by the interpreter.
FILESERVER_URL_PREFIX=/api/files

# Temperature for sampling from the model.
# LLM_TEMPERATURE=

# Maximum number of tokens to generate.
# LLM_MAX_TOKENS=

# The number of similar embeddings to return when retrieving documents.
TOP_K=3

VECTOR_STORE_PROVIDER=chroma

# The directory to store the llamaindex's storage files.
STORAGE_DIR="storage/context"

# The name of the collection in your Chroma database
CHROMA_COLLECTION=default

# The API endpoint for your Chroma database
# CHROMA_HOST=

# The port for your Chroma database
# CHROMA_PORT=


# The local path to the Chroma database. 
# Specify this if you are using a local Chroma database. 
# Otherwise, use CHROMA_HOST and CHROMA_PORT config above
CHROMA_PATH="storage/chromadb"

# Custom system prompt.
# Example:
# SYSTEM_PROMPT="You are a helpful assistant who helps users with their questions."
# SYSTEM_PROMPT=